{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C-LIENet_Train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"d8pp25gfjpBV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609594675690,"user_tz":-330,"elapsed":2749,"user":{"displayName":"PRAVEEN RAVIRATHINAM","photoUrl":"","userId":"18159518864572554415"}},"outputId":"429f6179-68d1-470d-e8c1-c45b050431ec"},"source":["import numpy as np \n","import os\n","import glob\n","%tensorflow_version 1.x\n","import skimage.io as io\n","import skimage.transform as trans\n","import tensorflow as tf\n","from keras import *\n","from keras import backend as K\n","from keras.callbacks import *\n","from keras.layers import *\n","from keras.models import *\n","from keras.optimizers import *\n","from keras.preprocessing.image import *\n","from keras.losses import *\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","import cv2\n","\n","from tensorflow.python.compat import compat\n","from tensorflow.python.framework import *\n","from tensorflow.python.ops import *\n","from tensorflow.python.util import *\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Iev4wxXeHgqF"},"source":["## PATHS AND EXPERIMENT INFO ##\r\n","train_data_image_folder = 'data/LID/train/high'\r\n","train_data_mask_folder = 'data/LID/train/low'\r\n","val_data_image_folder = 'data/LID/val/high'\r\n","val_data_mask_folder = 'data/LID/val/low'\r\n","model_folder = 'clienet/models'\r\n","\r\n","IMG_HEIGHT = 256\r\n","IMG_WIDTH = 256\r\n","NUM_EPOCHS = 60\r\n","BATCH_SIZE = 8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SKRKi3YJ3Q_"},"source":["## LOAD TRAIN AND VALIDATION IMAGES AND RESPECTIVE MASKS ##\r\n","train_images = []\r\n","train_masks = []\r\n","val_images = []\r\n","val_masks = []\r\n","\r\n","train_files = os.listdir(train_data_image_folder)\r\n","train_files = sorted([file for file in train_files if file.endswith(\".jpg\")])\r\n","\r\n","val_files = os.listdir(val_data_image_folder)\r\n","val_files = sorted([file for file in test_files if file.endswith(\".jpg\")])\r\n","\r\n","print(train_files)\r\n","print(val_files)\r\n","\r\n","# LOAD TRAIN IMAGES\r\n","for image in train_files:\r\n","\r\n","  print(image)\r\n","\r\n","  train_img = cv2.imread(os.path.join(train_data_image_folder, image))\r\n","  train_img = cv2.resize(train_img, (IMG_HEIGHT, IMG_WIDTH), interpolation = cv2.INTER_CUBIC)\r\n","  train_images.append(train_img)\r\n","\r\n","  train_mask = cv2.imread(os.path.join(train_data_mask_folder, image))\r\n","  train_mask = cv2.resize(train_mask, (IMG_HEIGHT, IMG_WIDTH), interpolation = cv2.INTER_CUBIC)\r\n","  train_masks.append(train_mask)\r\n","\r\n","# LOAD VAL IMAGES\r\n","for image in val_files:\r\n","\r\n","  print(image)\r\n","\r\n","  val_img = cv2.imread(os.path.join(val_data_image_folder, image))\r\n","  val_img = cv2.resize(val_img, (IMG_HEIGHT, IMG_WIDTH), interpolation = cv2.INTER_CUBIC)\r\n","  val_images.append(val_img)\r\n","\r\n","  val_mask = cv2.imread(os.path.join(val_data_mask_folder, image))\r\n","  val_mask = cv2.resize(val_mask, (IMG_HEIGHT, IMG_WIDTH), interpolation = cv2.INTER_CUBIC)\r\n","  val_masks.append(val_mask)\r\n","\r\n","# CREATE THE ARRAYS\r\n","train_image_array = np.array(train_images)\r\n","train_mask_array = np.array(train_masks)\r\n","val_image_array = np.array(val_images)\r\n","val_mask_array = np.array(val_masks)\r\n","\r\n","print(train_image_array.shape,train_mask_array.shape)\r\n","print(val_image_array.shape,val_mask_array.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9W0O1eN9M_X"},"source":["## DEFINE METRICS AND LOSS COMPONENTS ##\n","\n","from keras import backend as K\n","from keras.applications.vgg16 import VGG16\n","vggmodel = VGG16(include_top=False)\n","\n","# PEAK SIGNAL TO NOISE RATIO\n","def psnr(y_true, y_pred):\n","    return tf.image.psnr(y_true, y_pred, max_val=255.0)\n","\n","# STRUCTURAL SIMILARITY INDEX\n","def ssim(y_true, y_pred):\n","    return tf.image.ssim(y_true, y_pred, max_val=255.0)\n","\n","# ABSOLUTE BRIGHTNESS\n","def ab(y_true, y_pred):\n","    return K.abs(K.mean(y_true[:,:,:,:3])-K.mean(y_pred[:,:,:,:3]))\n","\n","# STRUCTURAL LOSS\n","def ssim_loss(y_true, y_pred):\n","\treturn 1-tf.image.ssim(y_true, y_pred, max_val=255.0)\n","\n","# PERCEPTUAL LOSS\n","def per_loss_vgg(img_true, img_generated):\n","\timage_shape = (256, 256, 3)\n","\tvgg = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)\n","\tloss_block1 = Model(inputs=vgg.input, outputs=vgg.get_layer('block1_conv2').output)\n","\tloss_block1.trainable = False\n","\tloss_block2 = Model(inputs=vgg.input, outputs=vgg.get_layer('block2_conv2').output)\n","\tloss_block2.trainable = False\n","\tloss_block3 = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n","\tloss_block3.trainable = False\n","\tnormalisation = 15 * 256 * 256\n","\tloss = \tK.mean(K.square(img_true - img_generated)) + \\\n","\t\t\t2 * K.mean(K.square(loss_block1(img_true) - loss_block1(img_generated))) + \\\n","\t\t\t4 * K.mean(K.square(loss_block2(img_true) - loss_block2(img_generated))) + \\\n","\t\t\t8 * K.mean(K.square(loss_block3(img_true) - loss_block3(img_generated)))\n","\treturn loss/normalisation\n","\n","# WEIGHTED PATCH-WISE EUCLIDEAN LOSS\n","def wpw(y_true,y_pred,weight = 4, percentage = 0.25, patches_per_row = 16):\n","\tgray_org = 0.39 * y_pred[:, :, :, 0] + 0.5 * y_pred[:, :, :, 1] + 0.11 * y_pred[:, :, :, 2]\n","\tgray_true = 0.39 * y_true[:, :, :, 0] + 0.5 * y_true[:, :, :, 1] + 0.11 * y_true[:, :, :, 2]\n","\tgray = tf.expand_dims(gray_org,-1)\n","\tpatch_length = int(gray_org.shape[1])/patches_per_row \n","\tno_of_patches = patches_per_row*patches_per_row\n","\tno_of_patches_to_consider = int(no_of_patches * percentage)\n","\tnormalization_factor = int(no_of_patches*((weight - 1)*percentage + 1)*int(gray_org.shape[1])*int(gray_org.shape[1]))\n","\tfilter_of_ones = tf.ones([patch_length,patch_length,1,1], tf.float32)\n","\tsum_of_patches = tf.nn.conv2d(gray, filter = filter_of_ones, strides= patch_length, padding= 'SAME')\n","\tsorted_sums = tf.sort(tf.reshape(sum_of_patches, shape = [-1]))\n","\tthreshold_sum = sorted_sums[no_of_patches_to_consider]\n","\tmask = tf.to_float(sum_of_patches <= threshold_sum)\n","\tweighted_mask_per_channel = tf.add(tf.multiply(float(weight),mask), tf.subtract(float(1),mask))\n","\tsquared_loss = tf.square(gray_org - gray_true)\n","\tsquared_loss = tf.expand_dims(squared_loss,-1)\n","\tsum_s = tf.reduce_sum(squared_loss)\n","\tfilter_of_ones_d = tf.ones([patch_length,patch_length,1,1], tf.float32)\n","\tsum_of_squared_loss_patches = tf.nn.conv2d(squared_loss, filter = filter_of_ones_d, strides= patch_length, padding= 'SAME')\n","\tloss = tf.reduce_sum(tf.multiply(weighted_mask_per_channel, sum_of_squared_loss_patches))\n","\treturn loss/normalization_factor\n","\n","# TOTAL LOSS\n","def total_loss(y_true,y_pred):\n","\tw_per = 1\n","\tw_ssim = 1\n","\tw_wpw = 0.1\n","\ttotal_loss = w_per * per_loss_vgg(y_true, y_pred) + w_ssim * (1 - ssim(y_true,y_pred)) + w_wpw * wpw(y_true, y_pred)\n"," \n","\treturn total_loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sueZ9ma_EG70"},"source":["## DEFINE MODEL COMPONENTS ##\n","\n","def _define_conv_block(\n","    input_, layers, filters,\n","    kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal', **kwargs):\n","    output_ = Conv2D(filters, kernel_size, activation = activation, padding = padding, kernel_initializer = kernel_initializer, **kwargs)(input_)\n","    for layer in range(1, layers):\n","        output_ = Conv2D(filters, kernel_size, activation = activation, padding = padding, kernel_initializer = kernel_initializer, **kwargs)(output_)\n","    return output_\n","\n","def _define_sep_conv_block(\n","    input_, layers, filters,\n","    kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal', **kwargs):\n","    output_ = SeparableConv2D(filters, kernel_size, activation = activation, padding = padding, kernel_initializer = kernel_initializer, **kwargs)(input_)\n","    for layer in range(1, layers):\n","        output_ = SeparableConv2D(filters, kernel_size, activation = activation, padding = padding, kernel_initializer = kernel_initializer, **kwargs)(output_)\n","    return output_\n","\n","def _define_aspp_block(input_, filters, dilation_rates=list((1, 3, 5, 7)),\n","                       kernel_size=3, padding='same', activation='relu', kernel_initializer='he_normal', **kwargs):\n","    num_parallel_outputs = len(dilation_rates)\n","    parallel_outputs = [None] * num_parallel_outputs\n","    for output_idx in range(num_parallel_outputs):\n","        dilation_rate = (dilation_rates[output_idx], dilation_rates[output_idx])\n","        parallel_outputs[output_idx] = SeparableConv2D(filters, kernel_size, dilation_rate = dilation_rate, activation = activation, padding = padding, kernel_initializer = kernel_initializer, **kwargs)(input_)\n","    output_ = concatenate(parallel_outputs, axis=3)\n","    return output_\n","\n","def encoder_decoder_with_aspp_blocks(input_shape = (256, 256, 3)):\n","    inputs = Input(input_shape)\n","    \n","    block1 = _define_conv_block(inputs, 2, 64)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(block1)\n","\n","    block2 = _define_aspp_block(pool1, 64)\n","    block2 = _define_sep_conv_block(block2, 2, 64)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(block2)\n","    \n","    block3 = _define_aspp_block(pool2, 128)\n","    block3 = _define_sep_conv_block(block3, 2, 128)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(block3)\n","    \n","    block4 = _define_aspp_block(pool3, 256)\n","    block4 = _define_sep_conv_block(block4, 2, 256)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(block4)\n","\n","    block5 = _define_aspp_block(pool4, 512)\n","    block5 = _define_sep_conv_block(block5, 4, 512)\n","    \n","    up6 = SeparableConv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(block5))\n","    merge6 = concatenate([block4, up6], axis = 3)\n","    block6 = _define_sep_conv_block(merge6, 2, 512)\n","\n","    up7 = SeparableConv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(block6))\n","    merge7 = concatenate([block3, up7], axis = 3)\n","    block7 = _define_sep_conv_block(merge7, 2, 256)\n","\n","    up8 = SeparableConv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(block7))\n","    merge8 = concatenate([block2, up8], axis = 3)\n","    block8 = _define_sep_conv_block(merge8, 2, 128)\n","\n","    up9 = SeparableConv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(block8))\n","    merge9 = concatenate([block1, up9], axis = 3)    \n","      \n","    block9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","    output = Conv2D(3, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(block9)\n","    model = Model(inputs = inputs, outputs = output)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"inewcHfi8UUI"},"source":["## BUILD MODEL ##\n","\n","# Model Compile\n","loss = total_loss\n","\n","# Configure Optimizer\n","learning_rate = 1e-4\n","optimizer = Adam(lr = learning_rate)\n","\n","# Others\n","metrics = [psnr, ssim, ab,per_loss_vgg, ssim_loss,wpw]\n","\n","# Build\n","model = encoder_decoder_with_aspp_blocks()\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L14f5nEIZQSs"},"source":["## TRAIN ##\n","if not os.path.exists(model_folder):\n","  os.makedirs(model_folder)\n","model_name = 'model_clienet_epoch-{epoch:03d}_val_loss-{val_loss:.4f}_psnr-{val_psnr:.3f}_ssim-{val_ssim:.3f}_ab-{val_ab:.3f}.h5'\n","checkpointer = ModelCheckpoint(os.path.join(model_folder, model_name), verbose=0, save_best_only=False, save_weights_only = True)\n","\n","model.fit(\n","    x=train_image_array,\n","    y=train_mask_array,\n","    batch_size=BATCH_SIZE,\n","    epochs=NUM_EPOCHS,\n","    verbose=1,\n","    callbacks=[checkpointer],\n","    validation_data=(val_image_array,val_mask_array)\n",")\n"],"execution_count":null,"outputs":[]}]}